**L’arbre de décisions** 

L’anatomie de l’arbre :

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\f.png)

Des algorithmes :

ID3 – C5.0 – CART – CHAID – MARS -CI 

Avantages des arbres :

  ![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\g.png)

Conclusions :

\-    Les arbres de décisions sont flexibles et simple à faire 

\-    Les algorithmes matchs avec les données et les questions 

\-    Permet de regarder le surapprentissage 



**Les ensembles** 

La combinaison des estimations :

\-    Moyenne des plusieurs estimations

\-    Plus efficace qu’une seule estimation 

\-    La diversité aide 

 ![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\h.png)

  

Conclusions : 

\-    Beaucoup d’estimations offres de meilleurs résultats qu’une seule

\-    La diversité des données et des modèles aident 

\-    Le jeu aléatoire à un rôle important 


 

 

**K-Nearest Neighbors** (Méthodes de classification de cases)

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\i.png)

Distance 

\-    J predictor variables 

\-    Une distance Euclidienne avec la dimension J (J-dimensional space)

\-    Overlap Metric 

\-    Reduce dimensions before k-NN

Choising K

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\j.png)

\-    How many neighbors ?

\-    More = Smoother

\-    Risk of random noise and misclassification 

\-    Can Weight Neighbors

\-    Many variations 

Conclusion: 

\-    K-NN est d’une conception simple 

\-    Une méthode de classification non paramétrique (nonpaeametric)

\-    Le choix de K est impactant sur les résultats 


 

 

**Naive Bays classifiers**

Une autre méthode de tri (Algorithme) est celle de Naive Bayes. Elle permet la classification des données à travers des cases bien précise 

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\k.png)

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\l.png)

\-    Malgré ça, la naïveté marche bien

\-    Est encore meilleure avec la préparation des données

\-    Le balancement des classes permet la transformation des données 

Conclusion :

\-    Naive Bayes est simple mais efficace

\-    Marche avec une variété de « predictors»

\-    Facilité d’interprétation des résultats 


 

 

**Artificial neural networks** 

ANN :

\-    Inspiré par les réseaux neurologiques 

\-    Les « Data Neurons » transmettent les données 

\-    Les connexions s’adaptent avec l’expérience 

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\m.png)

Uses 

![](D:\Msc2\5DATA\1.1 - Blended Learning  Understanding Data Science ( ~3h)\screens\n.png)

\-    Utilisé où les règles basées sur l’apprentissage ratent 

\-    Avoir une vision d’ordinateur

\-    La reconnaissance d’un discours

\-    Infer nonlinear functions 

Le process d’entrée de données est une sorte de boite noire où la donnée rentre, est cachée, et en ressort. 

Conclusion :

\-    La règle « infer » est très compliquée

\-    On apprend grâce a l’expérience

\-    On utilise un process de boite noire 

 

  

  


 

 